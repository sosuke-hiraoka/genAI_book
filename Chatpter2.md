***2章入力データの特徴量化***  
&emsp;生成AIモデルに関わらず機械学習モデルにおいて, 入力を扱う際には, 扱いたい対象を計算機で扱えるようにデータ化し, 機械学習モデルで扱うために適切な変換を実施し特徴量化する必要がある. 
ここで, 機械学習モデルで扱うという意味は, 行列計算や初等関数に代表されるような処理を適用して, より有用な特徴量うぃ構築するとう意味である.  
&emsp;画像や音声データ場合は, データ化と特徴量化に大きな差分はなく, 本質的な困難は生じない.(連続量で自然に扱いやすいという意味)　一方で, テキストデータの場合は機械学習モデルで扱うために乗り越えるべき本質的な困難が生じる. 
&emsp;まずデータ化に関す手は, テキストは物理現象として定式化できるものではなく, 離散的な記号といて扱う必要がある. 例えば, あらゆる文字を扱うためにUnicode15.0を使うと, 14万9,186文字になり, これを計算機で扱えるようにバイト列にするためにUTF-8などの文字符号化を
用いる必要がある.  
&emsp;特徴量化に関しては, 機械学習において有用な特徴量をどのように作ればいいかは全く非自明である. 機械学習において, 誤差逆伝搬法でモデルを学習するのは強力な手法だが, 自然言語は離散的な記号であり連続的な変形操作が適用できない. 微分操作ができないようにテキストデータを何かしらの方法で連続的な特徴量に
変化させたい. この部分が乗り越えるべき本質的な困難である. しかし, 連続量が作れればなんでもいいわけではなく, 人間にとって意味のある特徴量であることが好ましい. 

**2.1 入力データの特徴**  
テキストの場合は, データをどのように特徴量化するかを考える必要があることを述べた. 機械学習での取り扱いを考える場合に, 単純な豊富として局所表現(local representation)というワンホットベクトルが考えられる. ここでは例として, $V = \\{物理学, 数学, 人文学, 社会学\\}$を考えてみる:  
```math
\mathcal{l}_{物理学} =  \begin{pmatrix} 1 \\ 0 \\ 0 \\ 0
\end{pmatrix}, \quad \mathcal{l}_{数学} =  \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}, \quad \mathcal{l}_{人文学} =  \begin{pmatrix} 0 \\ 0 \\ 1 \\ 0 \end{pmatrix}, \quad \mathcal{l}_{社会学} =  \begin{pmatrix} 0 \\ 0 \\ 0 \\ 1 \end{pmatrix}
```
これらはベクトルなので, 連続量で扱えるものと仮定して線形変換や関数の適用などをすることで, 機械学習モデルで扱うことができる. しかし以下の観点から効果的とは言い難い.  
- トークンが増えると次元数が大きくなるので, 計算コストが大きくなる. 一般的に$N$個のトークンを線形変換する場合は$$\mathcal{O}(V^2N)$$の計算量になる
- トークンの類似性や関係性などの言語的な情報をまったく反映していないので特徴量として有益ではない. 例えば, 2つのトークンのEuclid距離は$\sqrt2$であり, 他の典型的な距離を基準に類似度を定義しても同じ類似度になってしまう. (最小距離が$sqrt 2$となる意味)  

これを解決するための方向性として, 分散表現がある. これは各次元に連続的に分布したベクトルとして表現するものである. 例えば, トークンの情報を表すのに必要な次元が$\mathbb{R}^{log_2V}$程度であると仮定して, 以下のような分散表現を考える：
```math
\mathcal{l}_{物理学} =  \begin{pmatrix} 0.7 \\ 0.3 
\end{pmatrix}, \quad \mathcal{l}_{数学} =  \begin{pmatrix} 0.8 \\ 0.2 \end{pmatrix}, \quad \mathcal{l}_{人文学} =  \begin{pmatrix} 0.1 \\ 0.9 \end{pmatrix}, \quad \mathcal{l}_{社会学} =  \begin{pmatrix}  -0.1 \\ 0.8 \end{pmatrix}
```
この場合, 
```math
\|\mathcal{l}_{物理学} - \mathcal{l}_{数学}\|_2 < \|\mathcal{l}_{物理学} - \mathcal{l}_{人文学}\|_2
```
であるから, 類似度が高いという表現ができる. このような分散表現を作ることができれば, 局所表現と比べて小さい次元でトークン情報を扱うことができるし、トークン間の類似性や関係性を表現できるので効果的である. 
このように, トークンを分散表現として使える特徴量に変換する操作を埋め込みと呼ぶ. 問題は, このような埋め込みをどのように獲得するかである. 
